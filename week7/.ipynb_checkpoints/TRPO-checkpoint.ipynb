{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from this repo: https://github.com/MEfeTiryaki/trpo\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import gym\n",
    "import scipy.optimize\n",
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_inputs, 64)\n",
    "        self.affine2 = nn.Linear(64, 64)\n",
    "        \n",
    "        self.action_mean = nn.Linear(64, num_outputs)\n",
    "        self.action_mean.weight.data.mul_(0.1)\n",
    "        self.action_mean.bias.data.mul_(0.0)\n",
    "        \n",
    "        self.action_log_std = nn.Parameter(torch.zeros(1, num_outputs))\n",
    "        \n",
    "        self.saved_actions = []\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.final_value = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.affine1(x))\n",
    "        x = torch.tanh(self.affine2(x))\n",
    "        \n",
    "        action_mean = self.action_mean(x)\n",
    "        action_log_std = self.action_log_std.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_log_std)\n",
    "        \n",
    "        return action_mean, action_log_std, action_std\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Value, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_inputs, 64)\n",
    "        self.affine2 = nn.Linear(64, 64)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "        self.value_head.weight.data.mul_(0.1)\n",
    "        self.value_head.bias.data.mul_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.affine1(x))\n",
    "        x = torch.tanh(self.affine2(x))\n",
    "        \n",
    "        state_values = self.value_head(x)\n",
    "        return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_entropy(std):\n",
    "    var = std.pow(2)\n",
    "    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n",
    "    return entropy.sum(1, keepdim=True)\n",
    "\n",
    "def normal_log_density(x, mean, log_std, std):\n",
    "    var = std.pow(2)\n",
    "    log_density = -(x - mean).pow(2) / (\n",
    "        2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n",
    "    return log_density.sum(1, keepdim=True)\n",
    "\n",
    "def get_flat_params_from(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params\n",
    "\n",
    "def set_flat_params_to(model, flat_params):\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "        prev_ind += flat_size\n",
    "        \n",
    "def get_flat_grad_from(net, grad_grad=False):\n",
    "    grads = []\n",
    "    for param in net.parameters():\n",
    "        if grad_grad:\n",
    "            grads.append(param.grad.grad.view(-1))\n",
    "        else:\n",
    "            grads.append(param.grad.view(-1))\n",
    "    flat_grad = torch.cat(grads)\n",
    "    return flat_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n",
    "    x = torch.zeros(b.size())\n",
    "    r = b - Avp(x)\n",
    "    p = r\n",
    "    rdotr = torch.dot(r, r)\n",
    "    \n",
    "    for i in range(nsteps):\n",
    "        _Avp = Avp(p)\n",
    "        alpha = rdotr / torch.dot(p, _Avp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * _Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        beta = new_rdotr / rdotr\n",
    "        p = r + beta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "Transition = namedtuple('Transition', ('state','action', 'mask', 'next_state', 'reward'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"saves transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self):\n",
    "        return Transition(*zip(*self.memory))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/joschu/modular_rl\n",
    "# http://www.johndcook.com/blog/standard_deviation/\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape):\n",
    "        self._n = 0\n",
    "        self._M = np.zeros(shape)\n",
    "        self._S = np.zeros(shape)\n",
    "        \n",
    "    def push(self, x):\n",
    "        x = np.asarray(x)\n",
    "        assert x.shape == self._M.shape\n",
    "        self._n += 1\n",
    "        if self._n == 1:\n",
    "            self._M[...] = x\n",
    "        else:\n",
    "            oldM = self._M.copy()\n",
    "            self._M[...] = oldM + (x - oldM) / self._n\n",
    "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
    "            \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self_n\n",
    "        \n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._M\n",
    "    \n",
    "    @property\n",
    "    def var(self):\n",
    "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
    "    \n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZFilter:\n",
    "    \"\"\"\n",
    "    y = (x - mean) / std\n",
    "    using running estimates of mean, std\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
    "        self.demean= demean\n",
    "        self.destd = destd\n",
    "        self.clip = clip\n",
    "        \n",
    "        self.rs = RunningStat(shape)\n",
    "        \n",
    "    def __call__(self, x, update=True):\n",
    "        if update:\n",
    "            self.rs.push(x)\n",
    "        if self.demean:\n",
    "            x = x - self.rs.mean\n",
    "        if self.destd:\n",
    "            x = x / (self.rs.std + 1e-8)\n",
    "        if self.clip:\n",
    "            x = np.clip(x, -self.clip, self.clip)\n",
    "        return x\n",
    "    \n",
    "    def output_shape(self, input_space):\n",
    "        return input_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(model, f, x, fullstep, expected_improve_rate, max_backtracks=10,\n",
    "              accept_ratio=.1):\n",
    "    fval = f(True).data\n",
    "    print(\"fval before\", fval.item())\n",
    "    for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n",
    "        xnew = x + stepfrac * fullstep\n",
    "        set_flat_params_to(model, xnew)\n",
    "        newfval = f(True).data\n",
    "        actual_improve = fval - newfval\n",
    "        expected_improve = expected_improve_rate * stepfrac\n",
    "        ratio = actual_improve / expected_improve\n",
    "        print(\"a/e/r\", actual_improve.item(), expected_improve.item(), ratio.item())\n",
    "        \n",
    "        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n",
    "            print(\"fval after\", newfval.item())\n",
    "            return True, xnew\n",
    "    return False, x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trpo_step(model, get_loss, get_kl, max_kl, damping):\n",
    "    loss = get_loss()\n",
    "    grads = torch.autograd.grad(loss, model.parameters())\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
    "    \n",
    "    def Fvp(v):\n",
    "        kl = get_kl()\n",
    "        kl = kl.mean()\n",
    "        \n",
    "        grads = torch.autograd.grad(kl, model.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "        \n",
    "        kl_v = (flat_grad_kl * Variable(v)).sum()\n",
    "        grads = torch.autograd.grad(kl_v, model.parameters())\n",
    "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n",
    "        \n",
    "        return flat_grad_grad_kl + v * damping\n",
    "    \n",
    "    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n",
    "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "    \n",
    "    lm = torch.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm[0]\n",
    "    \n",
    "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
    "    print((\"lagrange multiplier:\", lm[0], \"grad_norm:\", loss_grad.norm()))\n",
    "    \n",
    "    prev_params = get_flat_params_from(model)\n",
    "    success, new_params = linesearch(model, get_loss, prev_params, fullstep, neggdotstepdir / lm[0])\n",
    "    set_flat_params_to(model, new_params)\n",
    "    \n",
    "    return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).unsqueeze(0)\n",
    "    action_mean, _, action_std = policy_net(Variable(state))\n",
    "    action = torch.normal(action_mean, action_std)\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_params(batch):\n",
    "    rewards = torch.Tensor(batch.reward)\n",
    "    masks = torch.Tensor(batch.mask)\n",
    "    actions = torch.Tensor(np.concatenate(batch.action, 0))\n",
    "    states = torch.Tensor(batch.state)\n",
    "    values = value_net(Variable(states))\n",
    "    \n",
    "    returns = torch.Tensor(actions.size(0), 1)\n",
    "    deltas = torch.Tensor(actions.size(0), 1)\n",
    "    advantages = torch.Tensor(actions.size(0), 1)\n",
    "    \n",
    "    prev_return = 0\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    \n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        returns[i] = rewards[i] + gamma * prev_return * masks[i]\n",
    "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values.data[i]\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "        \n",
    "        prev_return = returns[i, 0]\n",
    "        prev_value = values.data[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "        \n",
    "    targets = Variable(returns)\n",
    "        \n",
    "    def get_value_loss(flat_params):\n",
    "        set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
    "        for param in value_net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.fill_(0)\n",
    "                \n",
    "        values_ = value_net(Variable(states))\n",
    "        value_loss = (values_ - targets).pow(2).mean() # mean squared error\n",
    "        \n",
    "        # weight decay\n",
    "        for param in value_net.parameters():\n",
    "            value_loss += param.pow(2).sum() * l2_reg\n",
    "        value_loss.backward()\n",
    "        return (value_loss.data.double().numpy(), get_flat_grad_from(value_net).data.double().numpy())\n",
    "    \n",
    "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, \n",
    "                                get_flat_params_from(value_net).double().numpy(), maxiter=25)\n",
    "    set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
    "    \n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "    fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n",
    "\n",
    "    def get_loss(volatile=False):\n",
    "        if volatile:\n",
    "            with torch.no_grad():\n",
    "                action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "        else:\n",
    "            action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "            \n",
    "        log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n",
    "        action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n",
    "        return action_loss.mean()\n",
    "    \n",
    "    def get_kl():\n",
    "        mean1, log_std1, std1 = policy_net(Variable(states))\n",
    "        mean0 = Variable(mean1.data)\n",
    "        log_std0 = Variable(log_std1.data)\n",
    "        std0 = Variable(std1.data)\n",
    "        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "    \n",
    "    trpo_step(policy_net, get_loss, get_kl, max_kl, damping)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickmatthew/opt/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"BipedalWalker-v3\"\n",
    "seed = 543\n",
    "gamma = 0.995\n",
    "tau = 0.97\n",
    "l2_reg = 0.0001\n",
    "max_kl = 0.01\n",
    "damping = .1\n",
    "batch_size = 15000\n",
    "log_interval = 1\n",
    "render = True\n",
    "\n",
    "env = gym.make(env_name)\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "policy_net = Policy(num_inputs, num_actions)\n",
    "value_net = Value(num_inputs)\n",
    "\n",
    "running_state = ZFilter((num_inputs,), clip=5)\n",
    "running_reward = ZFilter((1,), demean=False, clip=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lagrange multiplier:', tensor(0.5765), 'grad_norm:', tensor(0.0542))\n",
      "fval before -1.7532360391341687e-18\n",
      "a/e/r 0.010656751657885214 0.011640831390811483 0.9154631056933753\n",
      "fval after -0.010656751657885216\n",
      "Episode 1\tLast Reward: -114.51168962065101\t Average Reward -114.72\n",
      "('lagrange multiplier:', tensor(0.3720), 'grad_norm:', tensor(0.0356))\n",
      "fval before -9.806724436172494e-18\n",
      "a/e/r 0.007940812518537918 0.007586430311059919 1.0467126425667366\n",
      "fval after -0.007940812518537928\n",
      "Episode 2\tLast Reward: -124.593293644332\t Average Reward -112.17\n",
      "('lagrange multiplier:', tensor(0.5592), 'grad_norm:', tensor(0.0616))\n",
      "fval before -1.0333800612627887e-17\n",
      "a/e/r 0.010557414354480339 0.011131448225994989 0.9484313397627702\n",
      "fval after -0.010557414354480349\n",
      "Episode 3\tLast Reward: -101.70240413587075\t Average Reward -111.17\n",
      "('lagrange multiplier:', tensor(0.6933), 'grad_norm:', tensor(0.0918))\n",
      "fval before -2.3543496877405573e-18\n",
      "a/e/r 0.014395014155051459 0.013882288213480845 1.036933820540674\n",
      "fval after -0.01439501415505146\n",
      "Episode 4\tLast Reward: -115.02133244362567\t Average Reward -109.51\n",
      "('lagrange multiplier:', tensor(0.8576), 'grad_norm:', tensor(0.0911))\n",
      "fval before -1.5043709348710315e-17\n",
      "a/e/r 0.018439749204262054 0.017336043664807996 1.0636653645315033\n",
      "fval after -0.018439749204262067\n",
      "Episode 5\tLast Reward: -123.41235596057403\t Average Reward -112.70\n",
      "('lagrange multiplier:', tensor(1.3204), 'grad_norm:', tensor(0.1319))\n",
      "fval before -1.0832792175558465e-17\n",
      "a/e/r 0.05873125747598492 0.026303698262630345 2.232813686105288\n",
      "fval after -0.058731257475984934\n",
      "Episode 6\tLast Reward: -111.8871847191522\t Average Reward -113.58\n",
      "('lagrange multiplier:', tensor(0.6100), 'grad_norm:', tensor(0.0613))\n",
      "fval before 4.42650595414964e-19\n",
      "a/e/r 0.011054887163520698 0.012392537864254306 0.8920599867931815\n",
      "fval after -0.011054887163520698\n",
      "Episode 7\tLast Reward: -113.77838572475751\t Average Reward -111.96\n",
      "('lagrange multiplier:', tensor(0.6837), 'grad_norm:', tensor(0.0717))\n",
      "fval before 3.9601713040877516e-18\n",
      "a/e/r 0.01365282740894759 0.013728765167351623 0.9944687116810316\n",
      "fval after -0.013652827408947587\n",
      "Episode 8\tLast Reward: -105.05991939706807\t Average Reward -110.55\n",
      "('lagrange multiplier:', tensor(0.6782), 'grad_norm:', tensor(0.0723))\n",
      "fval before -9.880984919163893e-18\n",
      "a/e/r 0.015573599026843167 0.013453398950715087 1.157595867326553\n",
      "fval after -0.015573599026843177\n",
      "Episode 9\tLast Reward: -100.52364932595039\t Average Reward -106.28\n",
      "('lagrange multiplier:', tensor(0.4545), 'grad_norm:', tensor(0.0730))\n",
      "fval before 7.549516567451065e-18\n",
      "a/e/r 0.009341359800379746 0.009099132658093343 1.0266209045837957\n",
      "fval after -0.009341359800379739\n",
      "Episode 10\tLast Reward: -94.09248730823732\t Average Reward -99.01\n",
      "('lagrange multiplier:', tensor(0.5718), 'grad_norm:', tensor(0.0743))\n",
      "fval before -3.552713678800501e-18\n",
      "a/e/r 0.011213187187686461 0.011400175918552795 0.9835977328593652\n",
      "fval after -0.011213187187686465\n",
      "Episode 11\tLast Reward: -95.49365953444102\t Average Reward -93.70\n",
      "('lagrange multiplier:', tensor(0.5483), 'grad_norm:', tensor(0.0790))\n",
      "fval before 8.881784197001253e-19\n",
      "a/e/r 0.0110310608032645 0.01098512517248234 1.0041816210612902\n",
      "fval after -0.011031060803264499\n",
      "Episode 12\tLast Reward: -91.25720023406053\t Average Reward -88.15\n",
      "('lagrange multiplier:', tensor(0.8407), 'grad_norm:', tensor(0.0808))\n",
      "fval before 4.933410959267498e-19\n",
      "a/e/r 0.015646038270077895 0.0172978585711779 0.9045072374535247\n",
      "fval after -0.015646038270077895\n",
      "Episode 13\tLast Reward: -95.18732690173482\t Average Reward -92.58\n",
      "('lagrange multiplier:', tensor(0.7386), 'grad_norm:', tensor(0.0876))\n",
      "fval before -4.440892098500626e-19\n",
      "a/e/r 0.012104399731224213 0.01470598977417287 0.8230931693208673\n",
      "fval after -0.012104399731224213\n",
      "Episode 14\tLast Reward: -84.04764025266041\t Average Reward -79.48\n",
      "('lagrange multiplier:', tensor(0.7277), 'grad_norm:', tensor(0.0762))\n",
      "fval before 1.703175245002096e-18\n",
      "a/e/r 0.011734687957756954 0.014654300417418206 0.8007675305884285\n",
      "fval after -0.011734687957756952\n",
      "Episode 15\tLast Reward: -81.74123966329519\t Average Reward -80.90\n",
      "('lagrange multiplier:', tensor(0.9756), 'grad_norm:', tensor(0.1057))\n",
      "fval before -1.0603137261855627e-17\n",
      "a/e/r 0.02021503601285071 0.019825138346405483 1.0196668320610192\n",
      "fval after -0.02021503601285072\n",
      "Episode 16\tLast Reward: -69.90160958204193\t Average Reward -81.92\n",
      "('lagrange multiplier:', tensor(0.6940), 'grad_norm:', tensor(0.0928))\n",
      "fval before 9.658940314238861e-18\n",
      "a/e/r 0.013871005077461147 0.013837306592426905 1.002435335576989\n",
      "fval after -0.013871005077461137\n",
      "Episode 17\tLast Reward: -73.78537374369681\t Average Reward -74.66\n",
      "('lagrange multiplier:', tensor(0.5209), 'grad_norm:', tensor(0.0721))\n",
      "fval before -3.7747582837255325e-18\n",
      "a/e/r 0.010188288686680107 0.010373916405231791 0.982106302836789\n",
      "fval after -0.01018828868668011\n",
      "Episode 18\tLast Reward: -68.00960049485072\t Average Reward -65.50\n",
      "('lagrange multiplier:', tensor(0.7743), 'grad_norm:', tensor(0.0847))\n",
      "fval before 5.865485794543223e-18\n",
      "a/e/r 0.0128756868263144 0.015920178940472183 0.8087652076310465\n",
      "fval after -0.012875686826314394\n",
      "Episode 19\tLast Reward: -53.39110279999078\t Average Reward -61.49\n",
      "('lagrange multiplier:', tensor(0.8178), 'grad_norm:', tensor(0.1030))\n",
      "fval before 1.218199827518875e-18\n",
      "a/e/r 0.01591740921915332 0.0169321882929231 0.940068049314458\n",
      "fval after -0.01591740921915332\n",
      "Episode 20\tLast Reward: -44.55445375585704\t Average Reward -61.37\n",
      "('lagrange multiplier:', tensor(0.9949), 'grad_norm:', tensor(0.1296))\n",
      "fval before 5.773159728050814e-18\n",
      "a/e/r 0.02227136335265665 0.020695812499543515 1.0761289682706532\n",
      "fval after -0.022271363352656642\n",
      "Episode 21\tLast Reward: -49.171910163597666\t Average Reward -55.27\n",
      "('lagrange multiplier:', tensor(1.0514), 'grad_norm:', tensor(0.1459))\n",
      "fval before -2.215461261412136e-18\n",
      "a/e/r 0.014371914175809457 0.022264465719206417 0.6455090527239349\n",
      "fval after -0.014371914175809459\n",
      "Episode 22\tLast Reward: -49.41125206315342\t Average Reward -53.84\n",
      "('lagrange multiplier:', tensor(0.6643), 'grad_norm:', tensor(0.0738))\n",
      "fval before -4.218847493575595e-18\n",
      "a/e/r 0.00849155498005348 0.01369431790862292 0.6200787097768915\n",
      "fval after -0.008491554980053483\n",
      "Episode 23\tLast Reward: -56.59275468234365\t Average Reward -51.06\n",
      "('lagrange multiplier:', tensor(0.7521), 'grad_norm:', tensor(0.1478))\n",
      "fval before -1.2434497875801754e-17\n",
      "a/e/r 0.015362771830470839 0.015507643297635487 0.9906580603909856\n",
      "fval after -0.01536277183047085\n",
      "Episode 24\tLast Reward: -37.55995834303424\t Average Reward -47.69\n",
      "('lagrange multiplier:', tensor(0.9144), 'grad_norm:', tensor(0.1103))\n",
      "fval before -7.542001501669044e-19\n",
      "a/e/r 0.014804245239895052 0.018237944277082353 0.8117277372372468\n",
      "fval after -0.014804245239895052\n",
      "Episode 25\tLast Reward: -27.93378100476153\t Average Reward -38.61\n",
      "('lagrange multiplier:', tensor(0.5991), 'grad_norm:', tensor(0.1022))\n",
      "fval before 4.853433987432378e-18\n",
      "a/e/r 0.01089027816208682 0.01202127251324528 0.9059172521118452\n",
      "fval after -0.010890278162086814\n",
      "Episode 26\tLast Reward: -19.730195949081214\t Average Reward -34.88\n",
      "('lagrange multiplier:', tensor(0.6836), 'grad_norm:', tensor(0.1008))\n",
      "fval before -2.220446049250313e-19\n",
      "a/e/r 0.015137004161583917 0.014034241755738159 1.0785765576109498\n",
      "fval after -0.015137004161583917\n",
      "Episode 27\tLast Reward: -16.554811196426943\t Average Reward -17.81\n",
      "('lagrange multiplier:', tensor(0.8319), 'grad_norm:', tensor(0.1889))\n",
      "fval before 4.8849813083506885e-18\n",
      "a/e/r 0.01656739747278168 0.016506105152661554 1.0037133121080501\n",
      "fval after -0.016567397472781676\n",
      "Episode 28\tLast Reward: -10.356734030324208\t Average Reward -10.43\n",
      "('lagrange multiplier:', tensor(0.7266), 'grad_norm:', tensor(0.1463))\n",
      "fval before 1.1698750702883757e-17\n",
      "a/e/r 0.013234455973684913 0.01501523333399775 0.8814019522240276\n",
      "fval after -0.013234455973684901\n",
      "Episode 29\tLast Reward: 18.20899207090836\t Average Reward -0.49\n",
      "('lagrange multiplier:', tensor(0.6196), 'grad_norm:', tensor(0.1365))\n",
      "fval before -7.549516567451065e-18\n",
      "a/e/r 0.013189882339773085 0.012874887386840702 1.0244658414064527\n",
      "fval after -0.013189882339773092\n",
      "Episode 30\tLast Reward: 32.23292537989692\t Average Reward 15.82\n",
      "('lagrange multiplier:', tensor(0.7311), 'grad_norm:', tensor(0.1594))\n",
      "fval before 6.661338147750939e-19\n",
      "a/e/r 0.01333582280825749 0.01379211046875899 0.966916762917825\n",
      "fval after -0.01333582280825749\n",
      "Episode 31\tLast Reward: 18.706910205131564\t Average Reward 20.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lagrange multiplier:', tensor(0.6470), 'grad_norm:', tensor(0.1322))\n",
      "fval before -5.861977570020827e-17\n",
      "a/e/r 0.012426681880380392 0.012694483984153333 0.9789040575333947\n",
      "fval after -0.012426681880380451\n",
      "Episode 32\tLast Reward: 42.87545658730558\t Average Reward 37.44\n",
      "('lagrange multiplier:', tensor(0.5821), 'grad_norm:', tensor(0.1408))\n",
      "fval before -2.2648549702353192e-17\n",
      "a/e/r 0.011257061700635338 0.011917952668799671 0.9445466023795768\n",
      "fval after -0.011257061700635361\n",
      "Episode 33\tLast Reward: 52.21992763319794\t Average Reward 52.11\n",
      "('lagrange multiplier:', tensor(0.6273), 'grad_norm:', tensor(0.1689))\n",
      "fval before 3.885780586188048e-18\n",
      "a/e/r 0.01235365259498061 0.012567956794145208 0.9829483660172644\n",
      "fval after -0.012353652594980606\n",
      "Episode 34\tLast Reward: 46.91998688753894\t Average Reward 60.17\n",
      "('lagrange multiplier:', tensor(0.4192), 'grad_norm:', tensor(0.0783))\n",
      "fval before 4.6388725520618364e-18\n",
      "a/e/r 0.0089455639270449 0.008679083810617 1.0307037150743859\n",
      "fval after -0.008945563927044894\n",
      "Episode 35\tLast Reward: -45.12193488568954\t Average Reward 17.95\n",
      "('lagrange multiplier:', tensor(0.6117), 'grad_norm:', tensor(0.1372))\n",
      "fval before -1.6329376449279592e-17\n",
      "a/e/r 0.011251284681473087 0.012013648067781917 0.9365418911884618\n",
      "fval after -0.011251284681473102\n",
      "Episode 36\tLast Reward: 73.32078936088925\t Average Reward 53.90\n",
      "('lagrange multiplier:', tensor(0.9050), 'grad_norm:', tensor(0.1842))\n",
      "fval before 4.710260097846206e-19\n",
      "a/e/r 0.017925745777735206 0.018464587817253453 0.9708175430260756\n",
      "fval after -0.017925745777735206\n",
      "Episode 37\tLast Reward: 85.31794813962625\t Average Reward 41.99\n",
      "('lagrange multiplier:', tensor(0.5627), 'grad_norm:', tensor(0.1101))\n",
      "fval before 5.773765973478029e-18\n",
      "a/e/r 0.011571163802060642 0.01189535930195931 0.9727460523327557\n",
      "fval after -0.011571163802060637\n",
      "Episode 38\tLast Reward: -68.02614194664031\t Average Reward 56.46\n",
      "('lagrange multiplier:', tensor(0.5682), 'grad_norm:', tensor(0.1104))\n",
      "fval before -4.996003610813204e-18\n",
      "a/e/r 0.012823872904016409 0.011443495538386602 1.1206254995249838\n",
      "fval after -0.012823872904016414\n",
      "Episode 39\tLast Reward: 62.5184086498996\t Average Reward 71.50\n",
      "('lagrange multiplier:', tensor(0.6268), 'grad_norm:', tensor(0.1320))\n",
      "fval before -2.065014825802791e-17\n",
      "a/e/r 0.012450352878733105 0.012774130303937743 0.9746536619322859\n",
      "fval after -0.012450352878733126\n",
      "Episode 40\tLast Reward: 61.91767841383976\t Average Reward 63.83\n",
      "('lagrange multiplier:', tensor(0.5026), 'grad_norm:', tensor(0.1030))\n",
      "fval before 4.440892098500626e-18\n",
      "a/e/r 0.010128641390641678 0.010097000564805567 1.003133685655758\n",
      "fval after -0.010128641390641673\n",
      "Episode 41\tLast Reward: 80.79540142719407\t Average Reward 81.22\n",
      "('lagrange multiplier:', tensor(0.5313), 'grad_norm:', tensor(0.1244))\n",
      "fval before -6.217248937900877e-18\n",
      "a/e/r 0.011488236572858112 0.011418602412594922 1.0060983085098385\n",
      "fval after -0.01148823657285812\n",
      "Episode 42\tLast Reward: 76.43673440149057\t Average Reward 84.95\n",
      "('lagrange multiplier:', tensor(0.5240), 'grad_norm:', tensor(0.1339))\n",
      "fval before 3.996802888650563e-18\n",
      "a/e/r 0.010790437459440813 0.01091085915295887 0.9889631337156983\n",
      "fval after -0.01079043745944081\n",
      "Episode 43\tLast Reward: 90.02862467523208\t Average Reward 93.33\n",
      "('lagrange multiplier:', tensor(0.5711), 'grad_norm:', tensor(0.1369))\n",
      "fval before 1.3766765505351941e-17\n",
      "a/e/r 0.011560807129045524 0.01153610194896609 1.0021415535497802\n",
      "fval after -0.01156080712904551\n",
      "Episode 44\tLast Reward: 100.28548115618152\t Average Reward 98.57\n",
      "('lagrange multiplier:', tensor(0.6481), 'grad_norm:', tensor(0.1785))\n",
      "fval before 8.881784197001253e-18\n",
      "a/e/r 0.013207563645267496 0.012877803184545972 1.0256068877584068\n",
      "fval after -0.013207563645267487\n",
      "Episode 45\tLast Reward: 93.74502991372343\t Average Reward 100.88\n",
      "('lagrange multiplier:', tensor(0.4804), 'grad_norm:', tensor(0.1243))\n",
      "fval before 2.220446049250313e-18\n",
      "a/e/r 0.009987965516071435 0.009864500917165599 1.0125160512369147\n",
      "fval after -0.009987965516071434\n",
      "Episode 46\tLast Reward: 113.74269603889763\t Average Reward 110.78\n",
      "('lagrange multiplier:', tensor(0.4862), 'grad_norm:', tensor(0.1331))\n",
      "fval before -7.993605777301126e-18\n",
      "a/e/r 0.009844003424683574 0.00995050295186584 0.9892970709422988\n",
      "fval after -0.009844003424683582\n",
      "Episode 47\tLast Reward: 107.34373854411383\t Average Reward 115.20\n",
      "('lagrange multiplier:', tensor(0.5223), 'grad_norm:', tensor(0.1269))\n",
      "fval before 3.996802888650563e-18\n",
      "a/e/r 0.010696165849374268 0.010906362510508782 0.9807271525284457\n",
      "fval after -0.010696165849374264\n",
      "Episode 48\tLast Reward: 103.9295619615454\t Average Reward 112.65\n",
      "('lagrange multiplier:', tensor(0.5348), 'grad_norm:', tensor(0.1149))\n",
      "fval before -2.7977620220553945e-17\n",
      "a/e/r 0.010926580046050204 0.010845028721186615 1.0075196965319484\n",
      "fval after -0.010926580046050231\n",
      "Episode 49\tLast Reward: 133.0127078298237\t Average Reward 125.51\n",
      "('lagrange multiplier:', tensor(0.5647), 'grad_norm:', tensor(0.0875))\n",
      "fval before 6.079249963724334e-18\n",
      "a/e/r 0.010584274335951362 0.011453557589577782 0.924103646676782\n",
      "fval after -0.010584274335951355\n",
      "Episode 50\tLast Reward: 129.69327344271935\t Average Reward 106.32\n",
      "('lagrange multiplier:', tensor(0.5670), 'grad_norm:', tensor(0.1301))\n",
      "fval before 8.104628079763642e-18\n",
      "a/e/r 0.012549951947935133 0.011970577844299531 1.0483998442824967\n",
      "fval after -0.012549951947935125\n",
      "Episode 51\tLast Reward: 128.13437200698095\t Average Reward 129.01\n",
      "('lagrange multiplier:', tensor(0.6419), 'grad_norm:', tensor(0.1909))\n",
      "fval before -5.3290705182007515e-18\n",
      "a/e/r 0.012011167158020536 0.012494083677533623 0.9613483844052166\n",
      "fval after -0.01201116715802054\n",
      "Episode 52\tLast Reward: 119.20800116422778\t Average Reward 128.61\n",
      "('lagrange multiplier:', tensor(0.5490), 'grad_norm:', tensor(0.1434))\n",
      "fval before 1.1102230246251566e-18\n",
      "a/e/r 0.011516263701214858 0.011720765886958383 0.9825521482370813\n",
      "fval after -0.011516263701214856\n",
      "Episode 53\tLast Reward: 131.43999598311692\t Average Reward 131.50\n",
      "('lagrange multiplier:', tensor(0.6107), 'grad_norm:', tensor(0.1277))\n",
      "fval before 2.6478208897339302e-18\n",
      "a/e/r 0.012583769977541364 0.013148248501032989 0.957068158283799\n",
      "fval after -0.01258376997754136\n",
      "Episode 54\tLast Reward: 136.15999989244472\t Average Reward 113.64\n",
      "('lagrange multiplier:', tensor(0.4865), 'grad_norm:', tensor(0.1151))\n",
      "fval before -8.43769498715119e-18\n",
      "a/e/r 0.010007921878145842 0.010005533902031351 1.0002386655362794\n",
      "fval after -0.01000792187814585\n",
      "Episode 55\tLast Reward: 130.70036494578858\t Average Reward 132.71\n",
      "('lagrange multiplier:', tensor(0.5295), 'grad_norm:', tensor(0.1378))\n",
      "fval before -3.7747582837255325e-18\n",
      "a/e/r 0.011156022044452082 0.010965904368959797 1.0173371633652428\n",
      "fval after -0.011156022044452086\n",
      "Episode 56\tLast Reward: 138.4292237484426\t Average Reward 138.10\n",
      "('lagrange multiplier:', tensor(0.5483), 'grad_norm:', tensor(0.1516))\n",
      "fval before -7.993605777301126e-18\n",
      "a/e/r 0.010386244712214943 0.01058974854831689 0.9807829397295471\n",
      "fval after -0.010386244712214951\n",
      "Episode 57\tLast Reward: 136.3179325318944\t Average Reward 137.79\n",
      "('lagrange multiplier:', tensor(0.5354), 'grad_norm:', tensor(0.1469))\n",
      "fval before 1.3322676295501879e-18\n",
      "a/e/r 0.010584577419900787 0.010846958145655625 0.9758106630235385\n",
      "fval after -0.010584577419900786\n",
      "Episode 58\tLast Reward: 143.35209935978412\t Average Reward 144.87\n",
      "('lagrange multiplier:', tensor(0.5576), 'grad_norm:', tensor(0.1609))\n",
      "fval before 3.996802888650563e-18\n",
      "a/e/r 0.010804958655423882 0.011004628479236027 0.981855832371907\n",
      "fval after -0.010804958655423878\n",
      "Episode 59\tLast Reward: 154.65924761755593\t Average Reward 149.92\n",
      "('lagrange multiplier:', tensor(0.4823), 'grad_norm:', tensor(0.1284))\n",
      "fval before -2.6645352591003758e-18\n",
      "a/e/r 0.009371502372904258 0.009770374603733267 0.959175339021638\n",
      "fval after -0.009371502372904262\n",
      "Episode 60\tLast Reward: 138.6704718283933\t Average Reward 145.46\n",
      "('lagrange multiplier:', tensor(0.5356), 'grad_norm:', tensor(0.1675))\n",
      "fval before -4.8849813083506885e-18\n",
      "a/e/r 0.010625567902304994 0.010876291871770201 0.9769476608000959\n",
      "fval after -0.010625567902304999\n",
      "Episode 61\tLast Reward: 150.3721479259174\t Average Reward 152.17\n",
      "('lagrange multiplier:', tensor(0.5396), 'grad_norm:', tensor(0.1656))\n",
      "fval before -3.552713678800501e-18\n",
      "a/e/r 0.01055131112451585 0.0107057696155747 0.985572406598948\n",
      "fval after -0.010551311124515853\n",
      "Episode 62\tLast Reward: 146.7024422539663\t Average Reward 156.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lagrange multiplier:', tensor(0.4357), 'grad_norm:', tensor(0.1399))\n",
      "fval before -6.439293542825908e-18\n",
      "a/e/r 0.008864121347936144 0.009210307490563167 0.9624131829494592\n",
      "fval after -0.008864121347936151\n",
      "Episode 63\tLast Reward: 148.90854949314382\t Average Reward 154.36\n",
      "('lagrange multiplier:', tensor(0.4837), 'grad_norm:', tensor(0.1481))\n",
      "fval before 4.440892098500626e-18\n",
      "a/e/r 0.009564948549295018 0.0101220188285948 0.9449645086881232\n",
      "fval after -0.009564948549295012\n",
      "Episode 64\tLast Reward: 154.41602001176022\t Average Reward 158.35\n",
      "('lagrange multiplier:', tensor(0.4849), 'grad_norm:', tensor(0.1437))\n",
      "fval before -8.881784197001253e-19\n",
      "a/e/r 0.00997601704904702 0.010305265308316972 0.9680504820187182\n",
      "fval after -0.009976017049047022\n",
      "Episode 65\tLast Reward: 154.95999772397587\t Average Reward 160.73\n",
      "('lagrange multiplier:', tensor(0.4925), 'grad_norm:', tensor(0.1555))\n",
      "fval before 2.6645352591003758e-18\n",
      "a/e/r 0.009823998368318822 0.009822608036417262 1.0001415440681747\n",
      "fval after -0.009823998368318819\n",
      "Episode 66\tLast Reward: 154.88392778702504\t Average Reward 159.43\n",
      "('lagrange multiplier:', tensor(0.6223), 'grad_norm:', tensor(0.2259))\n",
      "fval before -1.021405182655144e-17\n",
      "a/e/r 0.011653387120719068 0.01243491954865914 0.9371501822040863\n",
      "fval after -0.011653387120719078\n",
      "Episode 67\tLast Reward: 170.91469543247152\t Average Reward 165.00\n",
      "('lagrange multiplier:', tensor(0.7252), 'grad_norm:', tensor(0.1860))\n",
      "fval before -6.7082131618532336e-18\n",
      "a/e/r 0.013077031435857705 0.014440585439913492 0.9055748806217405\n",
      "fval after -0.013077031435857712\n",
      "Episode 68\tLast Reward: 160.61765949747394\t Average Reward 137.67\n",
      "('lagrange multiplier:', tensor(0.5647), 'grad_norm:', tensor(0.1699))\n",
      "fval before 1.7763568394002505e-18\n",
      "a/e/r 0.012094572179469241 0.011163820110724485 1.0833721843879078\n",
      "fval after -0.01209457217946924\n",
      "Episode 69\tLast Reward: 178.23494542548787\t Average Reward 166.98\n",
      "('lagrange multiplier:', tensor(0.4872), 'grad_norm:', tensor(0.1580))\n",
      "fval before 1.687538997430238e-17\n",
      "a/e/r 0.00965036275167414 0.009973876797973052 0.967563861790968\n",
      "fval after -0.009650362751674123\n",
      "Episode 70\tLast Reward: 166.1922562633365\t Average Reward 169.41\n",
      "('lagrange multiplier:', tensor(0.5710), 'grad_norm:', tensor(0.1884))\n",
      "fval before 0.0\n",
      "a/e/r 0.01106878714741057 0.011040936995275428 1.0025224446210552\n",
      "fval after -0.01106878714741057\n",
      "Episode 71\tLast Reward: 152.8553983969661\t Average Reward 163.96\n",
      "('lagrange multiplier:', tensor(0.7915), 'grad_norm:', tensor(0.2429))\n",
      "fval before 1.199040866595169e-17\n",
      "a/e/r 0.012430058482952684 0.016644540139594394 0.7467949476948174\n",
      "fval after -0.012430058482952672\n",
      "Episode 72\tLast Reward: 162.89037601572494\t Average Reward 168.22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad264f263a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                     \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polyline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mdraw_polygon\u001b[0;34m(self, v, filled, **attrs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mgeom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0m_add_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_onetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36madd_onetime\u001b[0;34m(self, geom)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_onetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in count(1):\n",
    "    memory = Memory()\n",
    "    \n",
    "    num_steps = 0\n",
    "    reward_batch = 0\n",
    "    num_episodes = 0\n",
    "    while num_steps < batch_size:\n",
    "        state = env.reset()\n",
    "        state = running_state(state)\n",
    "        reward_sum = 0\n",
    "        \n",
    "        for t in range(10000):\n",
    "            action = select_action(state)\n",
    "            action = action.data[0].numpy()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            \n",
    "            next_state = running_state(next_state) # need to understand this line\n",
    "            \n",
    "            mask = 1\n",
    "            if done:\n",
    "                mask = 0\n",
    "            \n",
    "            memory.push(state, np.array([action]), mask, next_state, reward)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        num_steps += (t-1)\n",
    "        num_episodes += 1\n",
    "        reward_batch += reward_sum\n",
    "        \n",
    "    reward_batch /= num_episodes\n",
    "    batch = memory.sample()\n",
    "    update_params(batch)\n",
    "    \n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast Reward: {}\\t Average Reward {:.2f}'.format(\n",
    "        i_episode, reward_sum, reward_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
